# 产品需求文档 (PRD)：CrawlerMind - AI 智能爬虫平台

> **生成时间**：2025-02-16
> **基于路线图**：Agent 应用开发（JS/TS 路线）
> **产品阶段**：MVP 开发计划

---

## 一、执行摘要

### 1.1 产品概述

| 项目 | 内容 |
|------|------|
| **产品名称** | CrawlerMind (爬虫智脑) |
| **一句话描述** | AI 驱动的智能数据采集平台，让非技术人员也能获取任何网站数据 |
| **核心价值** | 无需编写代码，用自然语言描述需求，AI 自动分析、爬取、清洗数据 |
| **目标用户** | 企业运营、数据分析师、市场研究人员、中小型电商卖家 |
| **当前阶段**：MVP 开发计划 |

### 1.2 产品定位图

```
┌─────────────────────────────────────────────────────────┐
│                         用户痛点                          │
├─────────────────────────────────────────────────────────┤
│ • 传统爬虫需要写代码，技术门槛高                          │
│ • 网站结构变化快，爬虫频繁失效                            │
│ • 反爬机制复杂（IP封禁、验证码、动态加载）                 │
│ • 数据清洗耗时长，格式不统一                              │
│ • 外包开发成本高，周期长                                  │
└─────────────────────────────────────────────────────────┘
                            ↓ AI 解决
┌─────────────────────────────────────────────────────────┐
│                         CrawlerMind 方案                  │
├─────────────────────────────────────────────────────────┤
│ • 自然语言描述需求，无需编程                              │
│ • AI 自适应分析页面结构，自动应对变化                      │
│ • 智能反爬对抗（代理池、请求模拟、节奏控制）                │
│ • 自动数据清洗与标准化输出                                │
│ • SaaS 订阅模式，即开即用                                  │
└─────────────────────────────────────────────────────────┘
                            ↓ 创造
┌─────────────────────────────────────────────────────────┐
│                         用户价值                          │
├─────────────────────────────────────────────────────────┤
│ • 降低 90% 技术门槛，运营人员直接操作                      │
│ • 缩短 80% 开发周期，从周级到小时级                        │
│ • 节省 70% 成本，相比外包开发                              │
│ • 数据准确率提升，自动清洗去重                             │
└─────────────────────────────────────────────────────────┘
```

### 1.3 验证学习的核心技能

| 学习路线技能 | 产品中的应用形式 | 验收方式 |
|-------------|-----------------|---------|
| **LLM 调用与流式输出** | 自然语言需求解析、实时状态反馈 | 用户输入需求后，Agent 能理解并展示分析过程 |
| **Function Calling** | 动态调用分析工具、爬虫工具、清洗工具 | 根据任务自动选择合适的工具组合 |
| **Agent 编排** | 多 Agent 协作（分析Agent、爬虫Agent、清洗Agent） | 完整任务流程自动执行 |
| **RAG 检索** | 爬虫策略知识库、历史任务复用 | 相似任务自动复用策略 |
| **记忆系统** | 任务历史、用户偏好、网站特征记忆 | 再次爬取同一网站时更高效 |

---

## 二、市场分析

### 2.1 目标市场

**一级市场（核心市场）**
- **定义**：中小型电商卖家 + 市场研究公司
- **规模**：中国电商卖家超 1000 万，有数据采集需求的约 10%
- **特征**：有明确数据需求但无技术团队，预算有限但愿意为效率付费

**二级市场（扩展市场）**
- **定义**：企业运营人员、数据分析师、内容创作者
- **规模**：约 200 万潜在用户
- **特征**：日常需要竞品数据、行业数据、舆情数据

### 2.2 竞品分析

| 竞品 | 核心功能 | 定价 | 优势 | 劣势 | 我们的差异化 |
|------|---------|------|------|------|-------------|
| **八爪鱼** | 可视化爬虫 | ¥299-2999/月 | 成熟稳定、用户基数大 | 需要手动配置、AI能力弱 | 全AI驱动，无需配置 |
| **后羿采集器** | 云爬虫 | ¥99-999/月 | 免费版可用、模板丰富 | 反爬能力弱、大网站受限 | 智能反爬对抗 |
| **Databrridge** | API服务 | 按调用量 | 数据质量高 | 价格昂贵、不支持自定义 | 价格亲民、支持任意网站 |
| **ScrapingBee** | API服务 | $49-249/月 | 反爬能力强 | 国内网站支持一般 | 国内网站优化 |

### 2.3 SWOT 分析

```
         内部因素                          外部环境
            ↓                                 ↓
┌───────────────────┐              ┌───────────────────┐
│  Strengths (优势)  │              │  Opportunities    │
│  • 10年爬虫经验    │              │  • 数据要素市场化 │
│  • RPA背景+反爬    │              │  • AI应用爆发年   │
│  • 全栈开发能力    │              │  • 中小企业数字化转型 │
│  • 3090本地模型    │              │  • 企业降本需求   │
└───────────────────┘              └───────────────────┘

┌───────────────────┐              ┌───────────────────┐
│  Weaknesses (劣势) │              │  Threats (威胁)   │
│  • 初创品牌认知度  │              │  • 大厂入场风险   │
│  • 服务器成本压力  │              │  • 法律合规风险   │
│  • 单人开发速度    │              │  • 网站反爬升级   │
└───────────────────┘              └───────────────────┘
```

---

## 三、产品功能

### 3.1 功能优先级矩阵

| 优先级 | 功能模块 | 描述 | 对应技能 | 开发估时 |
|--------|---------|------|---------|---------|
| 🔴 P0 | 自然语言任务解析 | 用户用自然语言描述爬取需求，AI 理解并生成方案 | LLM 调用、Prompt Engineering | 5天 |
| 🔴 P0 | 智能页面分析 | AI 自动识别页面结构、数据字段、列表规则 | Agent 编排、Tool Calling | 7天 |
| 🔴 P0 | 爬虫执行引擎 | 执行爬取任务，支持分页、翻页、深度控制 | 爬虫开发（你的强项） | 5天 |
| 🔴 P0 | 数据清洗与导出 | 自动去重、格式化，支持 JSON/CSV/Excel | 数据处理 | 3天 |
| 🟡 P1 | 反爬对抗系统 | 代理池、请求模拟、节奏控制、验证码识别 | 高级爬虫技术 | 7天 |
| 🟡 P1 | 定时任务 | 支持定时执行、增量更新 | 任务调度 | 3天 |
| 🟡 P1 | 任务模板市场 | 热门网站的一键爬取模板 | RAG + 向量检索 | 5天 |
| 🟢 P2 | Webhook 推送 | 爬取完成后推送到用户系统 | 集成能力 | 2天 |
| 🟢 P2 | 数据可视化 | 简单的数据图表展示 | 前端可视化 | 4天 |

### 3.2 MVP 功能范围（P0）

**MVP 目标**：验证「AI 自动爬虫」的核心价值——用户输入需求，自动完成爬取

| 功能 | 用户故事 | 验收标准 | 技术实现要点 |
|------|---------|---------|-------------|
| **自然语言需求解析** | 作为运营人员，我想要用自然语言描述我要采集的数据，以便不需要学习编程 | 用户输入"帮我抓取京东手机商品的前10页，包括商品名、价格、销量"，系统能正确识别目标网站、数据字段、分页规则 | 使用 Vercel AI SDK + GPT-4o，设计专门的 Prompt 模板 |
| **智能页面分析** | 作为系统，我需要自动分析网页结构，以便识别数据位置和分页方式 | 能正确识别 80% 的静态电商列表页结构 | 使用 Cheerio + AI 辅助分析，配合 CSS 选择器生成 |
| **爬虫执行** | 作为用户，我希望点击按钮就能开始爬取，并实时看到进度 | 点击"开始爬取"后，显示实时进度条和已抓取数量 | 使用 Crawlee，通过 Server-Sent Events 推送进度 |
| **数据导出** | 作为用户，我希望能导出 Excel 格式的数据 | 点击导出，下载包含所有字段的 xlsx 文件 | 使用 ExcelJS 库生成文件 |

### 3.3 后续迭代规划

**V1.0 后计划**：
- **V1.1**（MVP+2周）：反爬对抗系统 - 代理池 IP 轮换 + User-Agent 模拟
- **V1.2**（MVP+4周）：定时任务 - 支持 cron 表达式，增量更新
- **V2.0**（MVP+8周）：模板市场 - 热门网站预置模板，一键使用
- **V3.0**（MVP+12周）：AI 自动适配 - 网站结构变化后自动重新分析

---

## 四、用户体验设计

### 4.1 用户旅程地图

```
[访问官网] → [注册登录] → [输入爬取需求] → [AI分析方案] → [确认执行] → [查看进度] → [下载数据]
    ↓           ↓            ↓              ↓              ↓            ↓           ↓
  SEO优化    社交登录    自然语言输入    可视化预览      一键执行     实时进度条   多格式导出
```

### 4.2 核心页面/交互

| 页面/交互 | 目的 | 关键元素 | 对应技能点 |
|----------|------|---------|-----------|
| **首页** | 展示产品价值，引导转化 | 产品演示视频、核心优势、价格表 | - |
| **任务创建页** | 核心交互，输入需求 | 自然语言输入框、示例提示、AI 分析预览 | LLM 调用、流式输出 |
| **任务配置页** | AI 生成的方案确认 | 爬取范围预览、字段确认、分页规则确认 | Agent 编排 |
| **执行监控页** | 实时进度展示 | 进度条、成功/失败计数、实时日志 | 流式输出 |
| **数据预览页** | 结果查看与导出 | 数据表格、筛选排序、导出按钮 | 数据处理 |

### 4.3 用户体验原则

1. **渐进式披露** - 首次使用显示引导，后续使用直接进入任务创建
2. **即时反馈** - 每个操作都有视觉反馈，AI 思考过程可视化
3. **容错设计** - 任务失败可重试，部分成功可部分导出
4. **信任建立** - 明确展示数据来源、爬取时间、准确性

---

## 五、技术架构

### 5.1 技术栈选型

| 层级 | 技术选型 | 理由 | 对应学习技能 |
|------|---------|------|-------------|
| 前端 | Next.js 14 + Shadcn UI | 你熟悉的技术栈，快速开发 | React 前端开发 |
| AI SDK | Vercel AI SDK + GPT-4o | 路线图推荐，流式支持好 | LLM 调用、流式输出 |
| 爬虫框架 | Crawlee | 你已熟悉，功能强大 | 爬虫开发经验 |
| 网页解析 | Cheerio + Playwright | 静态/动态页面覆盖 | 你的爬虫经验 |
| 数据库 | PostgreSQL + pgvector | 任务存储 + 向量检索（模板市场） | 向量数据库 |
| 缓存 | Redis | 任务队列、进度缓存 | 异步编程 |
| 部署 | Vercel + Docker | 前端 Vercel，爬虫服务自托管 | 你的部署经验 |
| 存储 | MinIO / S3 | 爬取数据文件存储 | - |

### 5.2 系统架构图

```
┌─────────────────────────────────────────────────────────┐
│                       用户端                             │
│              Web (Next.js + Shadcn UI)                   │
└─────────────────────────────────────────────────────────┘
                            ↓
┌─────────────────────────────────────────────────────────┐
│                      API 层                              │
│           (Next.js API Routes / tRPC)                    │
└─────────────────────────────────────────────────────────┘
                            ↓
┌─────────────┬─────────────┬─────────────┬─────────────┐
│  Agent服务  │  爬虫服务   │  数据服务   │  用户服务   │
│  • 任务分析 │  • 页面抓取 │  • 清洗     │  • 认证     │
│  • 策略生成 │  • 数据提取 │  • 去重     │  • 订阅     │
│  • 模板匹配 │  • 反爬对抗 │  • 导出     │  • 配额     │
└─────────────┴─────────────┴─────────────┴─────────────┘
       ↓              ↓              ↓              ↓
┌─────────────┬─────────────┬─────────────┬─────────────┐
│  OpenAI API │  Crawlee    │  PostgreSQL │  Redis      │
│  (LLM调用)  │  (爬虫引擎) │  (数据存储) │  (队列缓存) │
└─────────────┴─────────────┴─────────────┴─────────────┘
```

### 5.3 数据模型（核心实体）

```typescript
// 任务实体
Task {
  id: string
  userId: string
  naturalQuery: string           // 用户的自然语言输入
  targetUrl: string              // 解析后的目标URL
  dataFields: DataField[]        // 要提取的字段
  paginationRule: PaginationRule // 分页规则
  status: 'analyzing' | 'ready' | 'running' | 'completed' | 'failed'
  resultCount: number
  createdAt: Date
  completedAt?: Date
}

// 数据字段
DataField {
  name: string                   // 字段名（如：商品名称）
  selector: string               // CSS选择器
  type: 'text' | 'image' | 'link' | 'number'
  required: boolean
}

// 爬取结果
CrawlResult {
  id: string
  taskId: string
  data: Record<string, any>[]    // 爬取的数据行
  exportedFormats: string[]      // 已导出的格式
  fileKeys: string[]             // 存储文件路径
}

// 模板（用于模板市场）
Template {
  id: string
  name: string
  description: string
  domain: string                 // 适用域名
  fields: DataField[]
  popularity: number             // 使用次数
}
```

### 5.4 技术风险与应对

| 风险 | 可能性 | 影响 | 应对措施 |
|------|--------|------|---------|
| 目标网站反爬升级 | 高 | 高 | 代理池 + 验证码识别服务 + 用户教育（自有数据） |
| LLM API 成本超预期 | 中 | 中 | 设置 Token 限制，缓存相似分析，考虑本地模型 |
| 并发任务性能问题 | 中 | 高 | 任务队列限流，水平扩展爬虫服务 |
| 法律合规风险 | 中 | 高 | 用户协议明确、只抓公开数据、提供 takedown 机制 |

---

## 六、商业模式

### 6.1 盈利模式

| 模式 | 描述 | 定价策略 | 目标占比 |
|------|------|---------|---------|
| **订阅制** | 按月/年订阅，不同套餐不同额度 | 分档定价 | 70% |
| **按量付费** | 超出额度后按抓取条数计费 | ¥0.01/条 | 20% |
| **企业定制** | 私有化部署 + 定制开发 | 面议 | 10% |

**推荐模式**：订阅制为主 - 现金流稳定，客户粘性高

### 6.2 定价策略

| 套餐 | 目标用户 | 价格 | 包含功能 | 限制 |
|------|---------|------|---------|------|
| **体验版** | 个人体验 | ¥0 | • 10个任务/月<br>• 1000条数据/月<br>• 仅CSV导出 | • 仅供个人使用<br>• 不支持定时 |
| **基础版** | 小微企业 | ¥199/月 | • 50个任务/月<br>• 5万条数据/月<br>• 全格式导出<br>• 定时任务 | • 单线程爬取<br>• 7天数据保留 |
| **专业版** | 中小企业 | ¥599/月 | • 200个任务/月<br>• 30万条数据/月<br>• 代理池<br>• API接入<br>• 30天数据保留 | • 3个并发任务 |
| **企业版** | 大型企业 | ¥1999/月起 | • 不限任务<br>• 100万条数据/月起<br>• 私有部署选项<br>• 专属技术支持 | • SLA保证 |

### 6.3 获客策略

**有机增长**：
- **内容营销** - "爬虫教程"、"反爬对抗技术"等技术文章吸引目标用户
- **SEO优化** - "网页采集器"、"数据抓取工具"等关键词
- **开源组件** - 开源部分爬虫工具，建立技术品牌

**付费增长**：
- **百度SEM** - "网页爬虫"、"数据采集"等关键词，预算 ¥5000/月
- **垂直社区** - 派代网、卖家论坛等电商社区投放

**合作伙伴**：
- **数据服务商合作** - 与数据清洗、BI 工具形成互补
- **培训机构合作** - 为数据分析培训提供工具支持

---

## 七、运营指标

### 7.1 北极星指标

**核心指标**：月度活跃爬取任务数 (Monthly Active Tasks)

**原因**：直接反映产品核心价值被使用的程度，比用户数更能反映健康度

### 7.2 分阶段指标

**阶段1：MVP 验证期（0-3个月）**

| 指标类型 | 指标 | 目标值 | 当前值 |
|---------|------|--------|--------|
| 获取 | 注册用户数 | 500 | - |
| 激活 | 创建首个任务比例 | 60% | - |
| 留存 | 次月留存率 | 30% | - |
| 变现 | 付费转化率 | 5% | - |

**阶段2：增长期（3-12个月）**

| 指标类型 | 指标 | 目标值 | 当前值 |
|---------|------|--------|--------|
| 获取 | 月新增用户 | 2000 | - |
| 激活 | 月活跃任务数 | 5000 | - |
| 留存 | 3月留存率 | 40% | - |
| 变现 | MRR | ¥10万 | - |

### 7.3 成本结构

| 成本项 | 月成本（¥） | 说明 |
|--------|------------|------|
| 服务器/云服务 | 2000 | Vercel Pro + 爬虫服务器 |
| 第三方服务 | 3000 | OpenAI API + 代理IP + 验证码识别 |
| 营销推广 | 5000 | SEM + 内容创作 |
| 其他 | 1000 | 域名、客服工具等 |
| **合计** | **11000** | |

### 7.4 收入预测（保守/中性/乐观）

| 时间 | 保守收入 | 中性收入 | 乐观收入 | 说明 |
|------|---------|---------|---------|------|
| 月3 | ¥2,000 | ¥5,000 | ¥10,000 | 10-50个付费用户 |
| 月6 | ¥8,000 | ¥20,000 | ¥40,000 | 40-200个付费用户 |
| 月12 | ¥30,000 | ¥80,000 | ¥200,000 | 150-1000个付费用户 |

---

## 八、开发计划

### 8.1 里程碑规划

| 阶段 | 目标 | 时间 | 交付物 | 状态 |
|------|------|------|--------|------|
| 需求确认 | 明确所有 MVP 需求 | Week 1 | 本 PRD 文档 | ✅ |
| 技术设计 | 架构设计 + 技术验证 | Week 2 | 架构图 + Demo | ⬜ |
| MVP开发 | 核心功能实现 | Week 3-8 | 可用版本 | ⬜ |
| 内测 | 邀请50用户测试 | Week 9-10 | 测试报告 | ⬜ |
| 公开上线 | 产品正式发布 | Week 11 | 正式版 v1.0 | ⬜ |
| 迭代优化 | 根据反馈迭代 | Week 12+ | v1.1, v1.2... | ⬜ |

### 8.2 技术任务分解

**Week 1-2：基础设施**
- [ ] Next.js 项目初始化 - 1天
- [ ] 数据库设计与 Migration - 2天
- [ ] 用户认证系统（NextAuth） - 2天

**Week 3-4：AI 核心能力**
- [ ] 自然语言需求解析（对应技能：LLM 调用） - 3天
- [ ] 页面结构分析 Agent（对应技能：Agent 编排） - 4天

**Week 5-6：爬虫执行**
- [ ] 爬虫执行引擎（对应技能：Function Calling） - 5天
- [ ] 进度实时推送（对应技能：流式输出） - 2天

**Week 7-8：数据与导出**
- [ ] 数据清洗模块 - 2天
- [ ] 多格式导出功能 - 2天
- [ ] 前端界面完善 - 4天

**Week 9-10：测试与修复**
- [ ] 内测用户招募与测试 - 7天
- [ ] Bug 修复与优化 - 7天

### 8.3 技能验证检查清单

| 学习路线技能 | 产品对应功能 | 完成标准 | 验证方式 |
|-------------|-------------|---------|---------|
| **LLM 调用** | 自然语言需求解析 | 能理解复杂需求并提取参数 | 测试各种自然语言输入 |
| **Function Calling** | 工具动态调用 | 根据任务类型调用对应工具 | 日志中能看到工具调用链 |
| **Agent 编排** | 多 Agent 协作流程 | 分析→执行→清洗自动完成 | 端到端任务测试 |
| **流式输出** | 实时进度展示 | 用户能看到爬取进度变化 | UI 体验测试 |
| **Prompt Engineering** | 需求解析准确率 | 复杂需求解析准确率>85% | 测试集验证 |

---

## 九、风险评估

### 9.1 风险矩阵

| 风险 | 可能性 | 影响 | 风险等级 | 应对策略 |
|------|--------|------|---------|---------|
| 法律合规风险 | 中 | 高 | 🟡 | 用户协议、只抓公开数据、提供申诉通道 |
| 竞品抄袭 | 高 | 中 | 🟡 | 快速迭代、建立技术壁垒、品牌建设 |
| 服务器成本失控 | 中 | 中 | 🟢 | 设置配额限制、监控成本、价格调整 |
| 大厂入场 | 低 | 高 | 🟢 | 聚焦细分市场、深度服务、差异化 |

### 9.2 止损条件

**项目暂停/终止的条件**：
1. **3个月内付费转化率 < 2%** - 说明产品-市场匹配度不够
2. **CAC > LTV/3** - 获客成本过高，商业模式不成立
3. **月活任务数连续2个月零增长** - 增长停滞，需要重新定位

---

## 十、附录

### 10.1 参考资料
- [Vercel AI SDK 文档](https://sdk.vercel.ai/docs)
- [Crawlee 爬虫框架](https://crawlee.dev)
- [中国数据安全法](https://flk.npc.gov.cn/detail2.html?ZmY4MDgwODE2ZjEzNWY0NzAxNmY0MTg2NjY5ZjA2NmQ)

### 10.2 术语表
| 术语 | 解释 |
|------|------|
| Selector | CSS选择器，用于定位HTML元素 |
| Anti-crawler | 反爬虫机制，网站用于防止自动抓取的技术 |
| User-Agent | HTTP头字段，标识客户端类型 |
| Proxy Pool | 代理池，多个代理服务器组成的集合 |

### 10.3 变更记录

| 版本 | 日期 | 变更内容 | 负责人 |
|------|------|---------|--------|
| v1.0 | 2025-02-16 | 初始版本 | AI生成 |

---

*文档版本：v1.0 | 最后更新：2025-02-16*
